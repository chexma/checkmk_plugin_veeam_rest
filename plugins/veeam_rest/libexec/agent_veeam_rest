#!/usr/bin/env python3
"""
Veeam Backup & Replication REST API Special Agent for Checkmk

This agent connects to the Veeam B&R REST API and collects monitoring data
for backup jobs, task sessions, repositories, proxies, and managed servers.

API Documentation:
https://helpcenter.veeam.com/references/vbr/13/rest/1.3-rev1/tag/SectionOverview
"""

import argparse
import json
import logging
import pickle
import sys
import time
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any

# Logger instance - configured in main()
logger = logging.getLogger("agent_veeam_rest")


# =============================================================================
# CACHING
# =============================================================================


@dataclass
class CachePerSection:
    """Cache intervals per section in seconds. None = no caching."""
    jobs: int | None = 300              # 5 minutes
    backup_objects: int | None = 300    # 5 minutes
    repositories: int | None = 1800     # 30 minutes
    proxies: int | None = 3600          # 1 hour
    managed_servers: int | None = 3600  # 1 hour
    license: int | None = 86400         # 24 hours
    server: int | None = 86400          # 24 hours
    scaleout_repositories: int | None = 1800  # 30 minutes
    wan_accelerators: int | None = 3600  # 1 hour
    malware_events: int | None = 300    # 5 minutes
    malware_combined: int | None = 300  # 5 minutes (for --malware-mode)


def get_cache_dir() -> Path:
    """Get cache directory, trying Checkmk paths first, fallback to /tmp."""
    try:
        from cmk.utils import paths
        cache_dir = paths.tmp_dir / "agents" / "agent_veeam_rest"
    except ImportError:
        cache_dir = Path("/tmp/check_mk_agent/agent_veeam_rest")
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def get_cache_path(hostname: str, section: str) -> Path:
    """Get path to cache file for a specific host/section."""
    return get_cache_dir() / f"{hostname}_{section}.pkl"


def load_cached_section(hostname: str, section: str, max_age: int) -> tuple[Any, int] | None:
    """Load cached section data if still valid.

    Returns (data, timestamp) tuple if cache is valid, None otherwise.
    """
    cache_path = get_cache_path(hostname, section)
    if not cache_path.exists():
        return None
    try:
        with open(cache_path, "rb") as f:
            cached = pickle.load(f)
        timestamp = cached["timestamp"]
        if timestamp + max_age >= int(time.time()):
            logger.info("Using cached data for section '%s' (age: %ds)",
                       section, int(time.time()) - timestamp)
            return cached["data"], timestamp
        logger.info("Cache expired for section '%s'", section)
    except (pickle.PickleError, KeyError, OSError) as e:
        logger.warning("Failed to load cache for '%s': %s", section, e)
    return None


def store_cached_section(hostname: str, section: str, data: Any) -> int:
    """Store section data in cache file. Returns timestamp."""
    timestamp = int(time.time())
    cache_path = get_cache_path(hostname, section)
    try:
        with open(cache_path, "wb") as f:
            pickle.dump({"timestamp": timestamp, "data": data}, f)
        logger.info("Cached section '%s'", section)
    except OSError as e:
        logger.warning("Failed to cache section '%s': %s", section, e)
    return timestamp

try:
    import requests
except ImportError:
    sys.stderr.write("Error: 'requests' module is required\n")
    sys.exit(2)

try:
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
except ImportError:
    pass


# =============================================================================
# REDACTION
# =============================================================================


class VeeamRedactor:
    """Redacts sensitive fields while maintaining consistency across all data.

    Same values are always redacted to the same placeholder, preserving
    relationships in the data (e.g., same hostname always becomes HOST_1).

    To add new fields to redact, simply add them to REDACT_FIELDS.
    """

    # Centralized configuration - easy to maintain
    # Format: field_name -> replacement prefix
    REDACT_FIELDS = {
        # Names
        "name": "NAME",
        "hostName": "HOST",
        "repositoryName": "REPO",
        "backupServer": "SERVER",
        "backupServerName": "SERVER",
        "componentName": "COMPONENT",
        "membership": "SOBR",
        # Paths
        "path": "PATH",
        "cacheFolder": "PATH",
        # Identifiers
        "id": "ID",
        "vbrId": "VBRID",
        "hostId": "HOSTID",
        "proxyId": "PROXYID",
        "repositoryId": "REPOID",
        "serverId": "SERVERID",
        "backupId": "BACKUPID",
        "sessionId": "SESSIONID",
        "parentSessionId": "SESSIONID",
        "objectId": "OBJID",
        # License & credentials
        "licensedTo": "ORG",
        "supportId": "SUPPORT",
        # Network
        "serverIps": "IP",
        # Descriptions (may contain sensitive info)
        "description": "DESC",
    }

    def __init__(self):
        self._mappings: dict[str, str] = {}  # original value -> redacted placeholder
        self._counter: int = 0  # global counter for unique IDs

    def _get_redacted(self, prefix: str, value: str) -> str:
        """Get consistent redacted value for a given original value.

        Same values always map to the same placeholder, regardless of which
        field they appear in. This preserves relationships in the data.
        """
        if not value or value in ("", None):
            return value

        # Check if we've seen this exact value before (any field)
        if value in self._mappings:
            return self._mappings[value]

        # Generate new placeholder with global counter
        self._counter += 1
        redacted = f"{prefix}_{self._counter}"

        # Cache the mapping by value only (not field-specific)
        self._mappings[value] = redacted
        return redacted

    def redact_value(self, field_name: str, value: Any) -> Any:
        """Redact a single value if field is in REDACT_FIELDS."""
        if field_name not in self.REDACT_FIELDS:
            return value

        prefix = self.REDACT_FIELDS[field_name]

        # Handle lists (e.g., serverIps)
        if isinstance(value, list):
            return [self._get_redacted(prefix, str(v)) for v in value]

        # Handle strings
        if isinstance(value, str):
            return self._get_redacted(prefix, value)

        return value

    def redact_dict(self, data: dict) -> dict:
        """Recursively redact all sensitive fields in a dict."""
        result = {}
        for key, value in data.items():
            if isinstance(value, dict):
                result[key] = self.redact_dict(value)
            elif isinstance(value, list):
                result[key] = self.redact_list(value)
            else:
                result[key] = self.redact_value(key, value)
        return result

    def redact_list(self, data: list) -> list:
        """Recursively redact all sensitive fields in a list."""
        result = []
        for item in data:
            if isinstance(item, dict):
                result.append(self.redact_dict(item))
            elif isinstance(item, list):
                result.append(self.redact_list(item))
            else:
                result.append(item)
        return result

    def redact(self, data: Any) -> Any:
        """Redact sensitive fields from any data structure."""
        if isinstance(data, dict):
            return self.redact_dict(data)
        elif isinstance(data, list):
            return self.redact_list(data)
        return data


# =============================================================================
# API CLIENT
# =============================================================================


class VeeamAPIError(Exception):
    """Custom exception for Veeam API errors."""
    pass


class VeeamRestAPI:
    """Veeam Backup & Replication REST API client with OAuth2 authentication."""

    API_VERSION = "1.3-rev1"

    def __init__(
        self,
        hostname: str,
        port: int,
        username: str,
        password: str,
        verify_ssl: bool = True,
        timeout: int = 60,
    ):
        self.base_url = f"https://{hostname}:{port}"
        self.verify_ssl = verify_ssl
        self.timeout = timeout
        self.session = requests.Session()
        self._authenticate(username, password)

    def _authenticate(self, username: str, password: str) -> None:
        """Acquire OAuth2 access token using password grant."""
        url = f"{self.base_url}/api/oauth2/token"
        logger.info("Authenticating to %s", self.base_url)

        try:
            start_time = time.time()
            response = self.session.post(
                url,
                data={
                    "grant_type": "password",
                    "username": username,
                    "password": password,
                },
                headers={
                    "Content-Type": "application/x-www-form-urlencoded",
                    "x-api-version": self.API_VERSION,
                },
                verify=self.verify_ssl,
                timeout=self.timeout,
            )
            response.raise_for_status()
            elapsed = (time.time() - start_time) * 1000
            logger.debug("Authentication request took %.0fms", elapsed)
            token_data = response.json()
            access_token = token_data["access_token"]
            self.session.headers.update({
                "Authorization": f"Bearer {access_token}",
                "x-api-version": self.API_VERSION,
                "Accept": "application/json",
            })
            logger.info("Authentication successful")
        except requests.exceptions.SSLError as e:
            raise VeeamAPIError(f"SSL Error: {e}")
        except requests.exceptions.ConnectionError as e:
            raise VeeamAPIError(f"Connection Error: {e}")
        except requests.exceptions.Timeout:
            raise VeeamAPIError(f"Timeout during authentication")
        except requests.exceptions.HTTPError as e:
            raise VeeamAPIError(f"Authentication failed: {e}")
        except (KeyError, json.JSONDecodeError) as e:
            raise VeeamAPIError(f"Invalid authentication response: {e}")

    def _get(self, endpoint: str, params: dict | None = None) -> Any:
        """Make authenticated GET request."""
        url = f"{self.base_url}{endpoint}"
        logger.info("GET %s", endpoint)

        try:
            start_time = time.time()
            response = self.session.get(
                url,
                params=params,
                verify=self.verify_ssl,
                timeout=self.timeout,
            )
            response.raise_for_status()
            elapsed = (time.time() - start_time) * 1000
            logger.debug("Request took %.0fms", elapsed)
            return response.json()
        except requests.exceptions.SSLError as e:
            raise VeeamAPIError(f"SSL Error: {e}")
        except requests.exceptions.ConnectionError as e:
            raise VeeamAPIError(f"Connection Error: {e}")
        except requests.exceptions.Timeout:
            raise VeeamAPIError(f"Request timeout for {endpoint}")
        except requests.exceptions.HTTPError as e:
            raise VeeamAPIError(f"HTTP Error for {endpoint}: {e}")
        except json.JSONDecodeError:
            raise VeeamAPIError(f"Invalid JSON response from {endpoint}")

    def _get_paginated(self, endpoint: str, params: dict | None = None, limit: int = 500) -> list[dict]:
        """Get all items from a paginated endpoint."""
        all_items = []
        skip = 0
        request_params = params.copy() if params else {}

        while True:
            request_params["skip"] = skip
            request_params["limit"] = limit

            response = self._get(endpoint, request_params)
            data = response.get("data", [])
            all_items.extend(data)

            pagination = response.get("pagination", {})
            total = pagination.get("total", len(data))
            logger.debug("Pagination: skip=%d, limit=%d, total=%d", skip, limit, total)

            if skip + len(data) >= total or not data:
                break

            skip += limit

        logger.info("Found %d items from %s", len(all_items), endpoint)
        return all_items

    # -------------------------------------------------------------------------
    # API Methods
    # -------------------------------------------------------------------------

    def get_server_info(self) -> dict:
        """Get backup server information."""
        return self._get("/api/v1/serverInfo")

    def get_job_states(self) -> list[dict]:
        """Get all job states."""
        return self._get_paginated("/api/v1/jobs/states")

    def get_sessions(self, created_after: str | None = None) -> list[dict]:
        """Get sessions, optionally filtered by creation time."""
        params = {}
        if created_after:
            params["createdAfterFilter"] = created_after
        return self._get_paginated("/api/v1/sessions", params)

    def get_task_sessions(self, created_after: str | None = None) -> list[dict]:
        """Get task sessions with performance details."""
        params = {}
        if created_after:
            params["createdAfterFilter"] = created_after
        return self._get_paginated("/api/v1/taskSessions", params)

    def get_repository_states(self) -> list[dict]:
        """Get all repository states."""
        return self._get_paginated("/api/v1/backupInfrastructure/repositories/states")

    def get_proxy_states(self) -> list[dict]:
        """Get all proxy states."""
        return self._get_paginated("/api/v1/backupInfrastructure/proxies/states")

    def get_managed_servers(self) -> list[dict]:
        """Get all managed servers."""
        return self._get_paginated("/api/v1/backupInfrastructure/managedServers")

    def get_license(self) -> dict:
        """Get license information."""
        return self._get("/api/v1/license")

    def get_scaleout_repositories(self) -> list[dict]:
        """Get all scale-out backup repositories."""
        return self._get_paginated("/api/v1/backupInfrastructure/scaleOutRepositories")

    def get_wan_accelerators(self) -> list[dict]:
        """Get all WAN accelerators."""
        return self._get_paginated("/api/v1/backupInfrastructure/wanAccelerators")

    def get_backup_objects(self) -> list[dict]:
        """Get all backup objects (VMs, agents, etc.)."""
        return self._get_paginated("/api/v1/backupObjects")

    def get_backups(self) -> list[dict]:
        """Get all backups.

        Returns backup metadata including jobId and name (job name).
        """
        return self._get_paginated("/api/v1/backups")

    def get_backup_objects_for_backup(self, backup_id: str) -> list[dict]:
        """Get backup objects for a specific backup."""
        return self._get_paginated(f"/api/v1/backups/{backup_id}/objects")

    def get_task_sessions_for_session(self, session_id: str, created_after: str | None = None) -> list[dict]:
        """Get task sessions for a specific parent session.

        Fetches task sessions filtered by time and filters by sessionId in Python.

        Args:
            session_id: The parent session ID to filter by.
            created_after: Optional timestamp to filter task sessions.

        Returns task sessions with: id, name, result, sessionId, etc.
        """
        params = {}
        if created_after:
            params["createdAfterFilter"] = created_after
        all_tasks = self._get_paginated("/api/v1/taskSessions", params)

        # Filter to only tasks belonging to this session
        return [t for t in all_tasks if t.get("sessionId") == session_id]

    def get_backup_objects_with_job_info(self, created_after: str | None = None) -> list[dict]:
        """Get all backup objects enriched with job name and warning info.

        Uses bulk endpoints and joins in Python for performance:
        - 1 call to /api/v1/backupObjects (all objects)
        - 1 call to /api/v1/backups (job name mapping)
        - 1 call to /api/v1/jobs/states (for warning detection)
        - 1 call to /api/v1/taskSessions (for warning jobs, filtered by time)
        Instead of 34+ individual calls per backup.

        Objects with no restore points are filtered out (typically VMs that
        were removed from the backup job but still have orphaned metadata).

        For jobs with Warning/Failed result, task sessions are checked to identify
        which specific VMs had warnings/errors (e.g., VSS failures).

        Args:
            created_after: Optional ISO timestamp to filter task sessions.
                          Only task sessions created after this time are fetched.
        """
        # Fetch all backups to build backupId -> jobName mapping (1 API call)
        backups = self.get_backups()
        backup_info = {
            b.get("id"): {"jobName": b.get("name"), "jobType": b.get("jobType")}
            for b in backups if b.get("id")
        }
        logger.debug("Built backup info mapping for %d backups", len(backup_info))

        # Fetch job states to identify warning/failed jobs and get session IDs
        jobs = self.get_job_states()
        job_by_name: dict[str, dict] = {}
        warning_session_ids: set[str] = set()  # Session IDs with Warning/Failed result
        for job in jobs:
            job_name = job.get("name")
            if job_name:
                job_result = job.get("lastResult")
                session_id = job.get("sessionId")
                job_by_name[job_name] = {
                    "lastResult": job_result,
                    "sessionId": session_id,
                }
                # Track sessions that had issues
                if job_result in ("Warning", "Failed") and session_id:
                    warning_session_ids.add(session_id)
        logger.debug("Built job state mapping for %d jobs, %d with warnings/failures",
                    len(job_by_name), len(warning_session_ids))

        # For warning/failed jobs: fetch task sessions to get per-VM results
        # Task sessions have result.result field that can be "Warning" or "Failed"
        issue_tasks_by_vm: dict[str, dict] = {}  # vm_name.lower() -> issue info
        if warning_session_ids:
            try:
                # Fetch recent task sessions with time filter (API doesn't support sessionId filter)
                all_tasks = self.get_task_sessions(created_after)
                logger.debug("Fetched %d task sessions (filtered)", len(all_tasks))

                # Filter to tasks from warning/failed sessions and check their results
                for task in all_tasks:
                    task_session_id = task.get("sessionId")
                    if task_session_id not in warning_session_ids:
                        continue

                    task_result = task.get("result", {})
                    task_result_status = task_result.get("result")
                    task_result_message = task_result.get("message", "")

                    # Only track tasks with Warning or Failed result
                    if task_result_status not in ("Warning", "Failed"):
                        continue

                    vm_name = task.get("name", "").lower()
                    if not vm_name:
                        continue

                    # Find the job name for this session
                    job_name_for_task = None
                    for jn, jd in job_by_name.items():
                        if jd.get("sessionId") == task_session_id:
                            job_name_for_task = jn
                            break

                    # Only overwrite if new issue is more severe (Failed > Warning)
                    existing = issue_tasks_by_vm.get(vm_name)
                    if not existing or (task_result_status == "Failed" and existing.get("severity") != "Failed"):
                        issue_tasks_by_vm[vm_name] = {
                            "warningMessage": task_result_message,
                            "warningTitle": task.get("name", ""),
                            "jobName": job_name_for_task or "",
                            "severity": task_result_status,  # "Warning" or "Failed"
                        }

                logger.info("Found %d VMs with warning/failed task results", len(issue_tasks_by_vm))
            except Exception as e:
                logger.warning("Failed to fetch task sessions for warning detection: %s", e)

        # Fetch all objects in bulk (1 API call instead of 34+)
        all_objects = self.get_backup_objects()
        logger.debug("Fetched %d backup objects in bulk", len(all_objects))

        # Enrich objects with job info from backup mapping
        result = []
        for obj in all_objects:
            # Skip objects without restore points (removed from job)
            if obj.get("restorePointsCount", 0) == 0:
                continue

            # Skip orphaned/outdated objects (old backups not part of any active job)
            # These typically have names starting with "outdated_" from Veeam
            obj_name_raw = obj.get("name", "")
            if obj_name_raw.lower().startswith("outdated_"):
                continue

            # Add job info from backup mapping (if object has backupId)
            backup_id = obj.get("backupId")
            if backup_id and backup_id in backup_info:
                obj["jobName"] = backup_info[backup_id]["jobName"]
                if backup_info[backup_id]["jobType"]:
                    obj["jobType"] = backup_info[backup_id]["jobType"]

            # Add warning info if this VM has a warning/failed task result
            obj_name = obj.get("name", "").lower()
            if obj_name in issue_tasks_by_vm:
                obj["warningInfo"] = issue_tasks_by_vm[obj_name]

            result.append(obj)

        logger.info("Enriched %d backup objects with job info", len(result))
        return result

    def get_object_restore_points(self, object_id: str) -> list[dict]:
        """Get restore points for a specific backup object."""
        return self._get_paginated(f"/api/v1/backupObjects/{object_id}/restorePoints")

    def get_all_restore_points(self, created_after: str | None = None) -> list[dict]:
        """Get all restore points from the backup server.

        This is much more efficient than calling get_object_restore_points()
        for each object individually. For 500 VMs, this reduces API calls
        from ~500 to ~1-5 (paginated).

        Args:
            created_after: Optional ISO 8601 timestamp to filter restore points.
                          Only returns restore points created after this time.

        Returns restore points with: id, name, creationTime, malwareStatus, backupId, etc.
        The 'name' field contains the object name for matching.
        """
        params = {}
        if created_after:
            params["createdAfterFilter"] = created_after
        return self._get_paginated("/api/v1/restorePoints", params)

    def get_malware_events(self, created_after: str | None = None) -> list[dict]:
        """Get malware detection events.

        Returns events with details about suspicious activity detection,
        including events marked as false positive (confirmed clean).
        """
        params = {}
        if created_after:
            params["createdAfterTimeUtcFilter"] = created_after
        return self._get_paginated("/api/v1/malwareDetection/events", params)

    def get_session_logs(self, session_id: str, status_filter: str | None = None) -> list[dict]:
        """Get logs for a session, optionally filtered by status.

        Args:
            session_id: The session ID to get logs for.
            status_filter: Optional filter for log status (None, Succeeded, Warning, Failed).

        Returns log records with: id, status, title (VM name), description (error message).
        """
        params = {}
        if status_filter:
            params["statusFilter"] = status_filter
        result = self._get(f"/api/v1/sessions/{session_id}/logs", params)
        return result.get("records", [])


# =============================================================================
# OUTPUT FUNCTIONS
# =============================================================================

def output_section(
    name: str,
    data: Any,
    redactor: VeeamRedactor | None = None,
    cache_timestamp: int | None = None,
    cache_interval: int | None = None,
) -> None:
    """Output agent section in JSON format, optionally with cache info.

    Args:
        name: Section name (without veeam_rest_ prefix)
        data: Section data to output as JSON
        redactor: Optional redactor for sensitive data
        cache_timestamp: Unix timestamp when data was cached
        cache_interval: Cache validity in seconds
    """
    header = f"<<<veeam_rest_{name}:sep(0)"
    if cache_timestamp is not None and cache_interval is not None:
        header += f":cached({cache_timestamp},{cache_interval})"
    header += ">>>"
    print(header)
    if redactor:
        data = redactor.redact(data)
    print(json.dumps(data, default=str))


def enrich_job_data(jobs: list[dict], server_name: str) -> list[dict]:
    """Enrich job data with additional fields."""
    now = datetime.now(timezone.utc)

    for job in jobs:
        job["backupServer"] = server_name

        # Calculate job age if lastRun exists
        last_run = job.get("lastRun")
        if last_run:
            try:
                last_run_dt = datetime.fromisoformat(last_run.replace("Z", "+00:00"))
                job["lastRunAgeSeconds"] = int((now - last_run_dt).total_seconds())
            except (ValueError, TypeError):
                job["lastRunAgeSeconds"] = None

    return jobs


def enrich_task_data(tasks: list[dict], server_name: str, job_type_lookup: dict[str, str] | None = None) -> list[dict]:
    """Enrich task session data with calculated fields.

    Args:
        tasks: List of task session dicts from API
        server_name: Backup server name
        job_type_lookup: Optional mapping of sessionId -> jobType
    """
    now = datetime.now(timezone.utc)

    for task in tasks:
        task["backupServer"] = server_name

        # Add job type if lookup provided (for filtering agent backups)
        if job_type_lookup:
            session_id = task.get("sessionId")
            if session_id and session_id in job_type_lookup:
                task["jobType"] = job_type_lookup[session_id]

        # Calculate backup age if endTime exists
        end_time = task.get("endTime")
        if end_time:
            try:
                end_time_dt = datetime.fromisoformat(end_time.replace("Z", "+00:00"))
                task["backupAgeSeconds"] = int((now - end_time_dt).total_seconds())
            except (ValueError, TypeError):
                task["backupAgeSeconds"] = None

        # Parse duration to seconds if available
        progress = task.get("progress", {})
        if progress:
            duration_str = progress.get("duration")
            if duration_str:
                try:
                    # Duration format: "HH:MM:SS" or "D.HH:MM:SS"
                    parts = duration_str.split(":")
                    if len(parts) == 3:
                        if "." in parts[0]:
                            day_hour = parts[0].split(".")
                            days = int(day_hour[0])
                            hours = int(day_hour[1])
                        else:
                            days = 0
                            hours = int(parts[0])
                        minutes = int(parts[1])
                        seconds = int(float(parts[2]))
                        task["durationSeconds"] = days * 86400 + hours * 3600 + minutes * 60 + seconds
                except (ValueError, IndexError):
                    task["durationSeconds"] = None

    return tasks


def enrich_backup_objects(
    api: VeeamRestAPI,
    objects: list[dict],
    server_name: str,
    tasks: list[dict] | None = None,
    all_restore_points: list[dict] | None = None,
) -> list[dict]:
    """Enrich backup objects with restore point info, task data, and calculated fields.

    For each backup object:
    - backupServer: The Veeam server name
    - backupAgeSeconds: Time since last backup (from restore points)
    - latestRestorePoint: Full restore point data
    - taskData: Task details for VM backups (duration, speed, bottleneck)

    Args:
        api: VeeamRestAPI instance
        objects: List of backup objects to enrich
        server_name: Name of the Veeam server
        tasks: Optional list of task sessions for task data enrichment
        all_restore_points: Optional pre-fetched restore points from get_all_restore_points().
                           If provided, uses bulk matching instead of per-object API calls.
                           This reduces ~500 API calls to ~1-5 for large environments.
    """
    now = datetime.now(timezone.utc)

    # Build task lookup by name (lowercase for case-insensitive matching)
    # Keep only the most recent task per object name
    task_by_name: dict[str, dict] = {}
    if tasks:
        for task in tasks:
            task_name = task.get("name", "").lower()
            if task_name:
                existing = task_by_name.get(task_name)
                if not existing or task.get("endTime", "") > existing.get("endTime", ""):
                    task_by_name[task_name] = task

    # Build restore point lookup by object name if bulk data provided
    # Key: object name (lowercase), Value: latest restore point
    rp_by_name: dict[str, dict] = {}
    if all_restore_points:
        for rp in all_restore_points:
            rp_name = rp.get("name", "").lower()
            if rp_name:
                existing = rp_by_name.get(rp_name)
                # Keep the latest restore point by creation time
                if not existing or rp.get("creationTime", "") > existing.get("creationTime", ""):
                    rp_by_name[rp_name] = rp

    for obj in objects:
        obj["backupServer"] = server_name
        object_id = obj.get("id")
        obj_name = obj.get("name", "").lower()

        # Try to merge task data if available (VM backups only)
        if obj_name and obj_name in task_by_name:
            task = task_by_name[obj_name]
            obj["taskData"] = {
                "state": task.get("state"),
                "result": task.get("result"),
                "progress": task.get("progress"),
                "durationSeconds": task.get("durationSeconds"),
                "sessionId": task.get("sessionId"),
                "endTime": task.get("endTime"),
            }

        # Get restore point data - either from bulk lookup or per-object API call
        latest: dict | None = None

        if all_restore_points is not None:
            # Use pre-fetched bulk data (fast path)
            if obj_name and obj_name in rp_by_name:
                latest = rp_by_name[obj_name]
        elif object_id:
            # Fallback to per-object API call (slow path)
            try:
                restore_points = api.get_object_restore_points(object_id)
                if restore_points:
                    latest = max(restore_points, key=lambda r: r.get("creationTime", ""))
            except VeeamAPIError:
                pass

        if latest:
            obj["latestRestorePoint"] = latest

            # Calculate backup age
            creation_time = latest.get("creationTime")
            if creation_time:
                try:
                    dt = datetime.fromisoformat(creation_time.replace("Z", "+00:00"))
                    obj["backupAgeSeconds"] = int((now - dt).total_seconds())
                except (ValueError, TypeError):
                    obj["backupAgeSeconds"] = None

    return objects


def output_piggyback_backup_objects(objects: list[dict], redactor: VeeamRedactor | None = None) -> None:
    """Output backup objects as piggyback sections for each VM/computer.

    Each backup object becomes a piggyback section attached to the hostname.
    Works for both VM backups and agent backups.
    """
    for obj in objects:
        hostname = obj.get("name")
        if not hostname:
            continue

        # Normalize hostname for Checkmk (spaces to underscores only)
        piggyback_hostname = hostname.replace(" ", "_")

        if redactor:
            obj = redactor.redact_dict(obj)

        # Start piggyback section
        print(f"<<<<{piggyback_hostname}>>>>")
        print("<<<veeam_rest_vm_backup:sep(0)>>>")
        print(json.dumps(obj))
        # End piggyback section
        print("<<<<>>>>")


def output_backup_objects_section(objects: list[dict], redactor: VeeamRedactor | None = None) -> None:
    """Output backup objects as regular section (services on Veeam server).

    Each object in the list becomes a separate service on the Veeam backup server.
    """
    if redactor:
        objects = [redactor.redact_dict(obj) for obj in objects]
    print("<<<veeam_rest_backup_objects:sep(0)>>>")
    print(json.dumps(objects))


def enrich_malware_events_with_backup_status(
    malware_events: list[dict],
    backup_objects: list[dict],
) -> dict[str, dict]:
    """Combine malware events with backup object malware status.

    Returns dict by hostname with:
    - machineName: Original machine name
    - events: list of malware events
    - malwareStatus: from latest restore point (if available)
    - restorePointsCount: number of restore points
    - lastBackup: creation time of latest restore point
    """
    result: dict[str, dict] = {}

    # Build lookup by hostname from backup objects (lowercase for matching)
    backup_by_name: dict[str, dict] = {}
    for obj in backup_objects:
        name = obj.get("name", "").lower()
        if name:
            backup_by_name[name] = obj

    # Group events by machine name
    for event in malware_events:
        machine = event.get("machine", {})
        # API uses "displayName" for the machine name, not "name"
        machine_name = machine.get("displayName") or machine.get("name", "Unknown")
        key = machine_name.lower()
        if key not in result:
            result[key] = {
                "machineName": machine_name,
                "events": [],
                "malwareStatus": None,
                "restorePointsCount": 0,
                "lastBackup": None,
            }
        result[key]["events"].append(event)

    # Enrich with backup object data
    for key, data in result.items():
        if key in backup_by_name:
            obj = backup_by_name[key]
            latest_rp = obj.get("latestRestorePoint", {})
            data["malwareStatus"] = latest_rp.get("malwareStatus")
            data["restorePointsCount"] = obj.get("restorePointsCount", 0)
            data["lastBackup"] = latest_rp.get("creationTime")

    return result


def output_malware_combined(
    malware_data: dict[str, dict],
    redactor: VeeamRedactor | None = None,
) -> None:
    """Output combined malware data as server-side section.

    Outputs all machines with malware events in a single section.
    """
    if redactor:
        malware_data = {k: redactor.redact_dict(v) for k, v in malware_data.items()}
    print("<<<veeam_rest_malware_events:sep(0)>>>")
    print(json.dumps(malware_data))


def output_piggyback_malware_combined(
    malware_data: dict[str, dict],
    redactor: VeeamRedactor | None = None,
) -> None:
    """Output combined malware data as piggyback sections per machine."""
    for key, data in malware_data.items():
        machine_name = data.get("machineName", key)
        piggyback_hostname = machine_name.replace(" ", "_")

        if redactor:
            data = redactor.redact_dict(data)

        print(f"<<<<{piggyback_hostname}>>>>")
        print("<<<veeam_rest_malware_events:sep(0)>>>")
        print(json.dumps(data))
        print("<<<<>>>>")


# =============================================================================
# MAIN
# =============================================================================

def parse_arguments() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Veeam Backup & Replication REST API Special Agent"
    )

    parser.add_argument(
        "--hostname",
        required=True,
        help="Veeam server hostname or IP"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=9419,
        help="REST API port (default: 9419)"
    )
    parser.add_argument(
        "--username",
        required=True,
        help="API username (format: DOMAIN\\user or user@domain)"
    )
    parser.add_argument(
        "--password",
        required=True,
        help="API password"
    )
    parser.add_argument(
        "--no-cert-check",
        action="store_true",
        help="Disable SSL certificate verification"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=60,
        help="API timeout in seconds (default: 60)"
    )
    parser.add_argument(
        "--sections",
        nargs="*",
        default=["jobs", "repositories", "proxies"],
        help="Sections to collect (default: jobs, repositories, proxies)"
    )
    parser.add_argument(
        "--backup-mode",
        choices=["piggyback-vms", "backup-server"],
        default=None,
        help="Backup service output mode: piggyback-vms (to VMs), backup-server (direct on server)"
    )
    parser.add_argument(
        "--malware-mode",
        choices=["piggyback-hosts", "backup-server"],
        default=None,
        help="Malware service output mode: piggyback-hosts (to affected hosts), backup-server (direct on server)"
    )
    parser.add_argument(
        "--session-age",
        type=int,
        default=86400,
        help="Max session/task age in seconds (default: 86400 = 24h)"
    )
    parser.add_argument(
        "--restore-points-days",
        type=int,
        default=7,
        help="Only fetch restore points from the last N days (default: 7, 0=all)"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="count",
        default=0,
        help="Increase verbosity (-v, -vv, -vvv)"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Show Python stack traces on errors"
    )
    parser.add_argument(
        "--redact",
        action="store_true",
        help="Redact sensitive values (hostnames, paths, IDs) for safe sharing"
    )
    parser.add_argument(
        "--cached-sections",
        type=str,
        default="",
        help="Comma-separated section:seconds pairs for caching (e.g., 'jobs:600,license:86400')"
    )
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="Disable all caching"
    )

    return parser.parse_args()


def parse_cache_config(cached_sections_arg: str, no_cache: bool) -> CachePerSection:
    """Parse --cached-sections argument into CachePerSection config.

    Args:
        cached_sections_arg: Comma-separated section:seconds pairs
        no_cache: If True, disable all caching

    Returns:
        CachePerSection with configured intervals
    """
    config = CachePerSection()

    if no_cache:
        # Disable all caching
        for field in config.__dataclass_fields__:
            setattr(config, field, None)
        return config

    if not cached_sections_arg:
        return config  # Use defaults

    # Parse custom intervals
    for pair in cached_sections_arg.split(","):
        pair = pair.strip()
        if ":" not in pair:
            continue
        section, interval_str = pair.split(":", 1)
        section = section.strip()
        try:
            interval = int(interval_str.strip())
            if hasattr(config, section):
                setattr(config, section, interval if interval > 0 else None)
        except ValueError:
            logger.warning("Invalid cache interval for '%s': %s", section, interval_str)

    return config


def main() -> int:
    args = parse_arguments()

    # Configure logging based on verbosity
    if args.verbose >= 3:
        log_level = logging.DEBUG
    elif args.verbose == 2:
        log_level = logging.INFO
    elif args.verbose == 1:
        log_level = logging.WARNING
    else:
        log_level = logging.CRITICAL  # Silent by default

    logging.basicConfig(
        level=log_level,
        format="%(asctime)s %(levelname)s %(message)s",
        stream=sys.stderr,
    )

    # Create redactor if --redact flag is set
    redactor = VeeamRedactor() if args.redact else None

    # Parse cache configuration
    cache_config = parse_cache_config(args.cached_sections, args.no_cache)
    hostname_for_cache = f"{args.hostname}_{args.port}"

    try:
        # Create API client
        api = VeeamRestAPI(
            hostname=args.hostname,
            port=args.port,
            username=args.username,
            password=args.password,
            verify_ssl=not args.no_cert_check,
            timeout=args.timeout,
        )

        logger.warning("Collecting %d sections...", len(args.sections))

        # Get server info for enrichment
        server_name = args.hostname
        if "server" in args.sections:
            try:
                cache_interval = cache_config.server
                cached = load_cached_section(hostname_for_cache, "server", cache_interval) if cache_interval else None
                if cached:
                    server_info, timestamp = cached
                    server_name = server_info.get("backupServerName", args.hostname)
                    output_section("server", server_info, redactor, timestamp, cache_interval)
                else:
                    server_info = api.get_server_info()
                    server_name = server_info.get("backupServerName", args.hostname)
                    timestamp = store_cached_section(hostname_for_cache, "server", server_info) if cache_interval else None
                    output_section("server", server_info, redactor, timestamp, cache_interval)
            except VeeamAPIError as e:
                sys.stderr.write(f"Warning: Failed to fetch server info: {e}\n")
        else:
            # Try to get server name for enrichment even if not collecting server section
            try:
                server_info = api.get_server_info()
                server_name = server_info.get("backupServerName", args.hostname)
            except VeeamAPIError:
                pass  # Use hostname as fallback

        # Calculate time filter for sessions/tasks
        created_after = None
        if args.session_age > 0:
            created_after = (
                datetime.now(timezone.utc) - timedelta(seconds=args.session_age)
            ).isoformat()

        if "jobs" in args.sections:
            try:
                cache_interval = cache_config.jobs
                cached = load_cached_section(hostname_for_cache, "jobs", cache_interval) if cache_interval else None
                if cached:
                    jobs, timestamp = cached
                    output_section("jobs", jobs, redactor, timestamp, cache_interval)
                else:
                    jobs = api.get_job_states()
                    jobs = enrich_job_data(jobs, server_name)
                    timestamp = store_cached_section(hostname_for_cache, "jobs", jobs) if cache_interval else None
                    output_section("jobs", jobs, redactor, timestamp, cache_interval)
            except VeeamAPIError as e:
                sys.stderr.write(f"Warning: Failed to fetch jobs: {e}\n")

        # Fetch and enrich backup objects once if needed by backup_mode or malware_mode
        # This avoids duplicate API calls for get_object_restore_points() (500+ calls per run)
        enriched_backup_objects: list[dict] | None = None
        if args.backup_mode or args.malware_mode:
            # Check cache for enriched backup objects
            cache_interval = cache_config.backup_objects
            cached = load_cached_section(hostname_for_cache, "backup_objects", cache_interval) if cache_interval else None

            if cached:
                enriched_backup_objects, _ = cached
            else:
                try:
                    backup_objects = api.get_backup_objects_with_job_info(created_after)

                    # Fetch restore points in bulk with optional time filter
                    # This is the major performance optimization for large environments
                    all_restore_points: list[dict] | None = None
                    try:
                        # Apply time filter if configured (default: 7 days)
                        rp_created_after: str | None = None
                        if args.restore_points_days > 0:
                            rp_created_after = (
                                datetime.now(timezone.utc) - timedelta(days=args.restore_points_days)
                            ).strftime("%Y-%m-%dT%H:%M:%SZ")
                            if args.debug:
                                sys.stderr.write(f"Filtering restore points to last {args.restore_points_days} days\n")

                        all_restore_points = api.get_all_restore_points(rp_created_after)
                        if args.debug:
                            sys.stderr.write(f"Fetched {len(all_restore_points)} restore points in bulk\n")
                    except VeeamAPIError as e:
                        # Fall back to per-object calls if bulk fetch fails
                        sys.stderr.write(f"Warning: Bulk restore points fetch failed, using per-object calls: {e}\n")

                    # Fetch tasks for enrichment (only available for VM backups)
                    tasks = None
                    if args.backup_mode:  # Only needed for backup services
                        try:
                            tasks = api.get_task_sessions(created_after)
                        except VeeamAPIError:
                            pass  # Tasks not available, continue without

                    enriched_backup_objects = enrich_backup_objects(
                        api, backup_objects, server_name, tasks, all_restore_points
                    )

                    # Cache the enriched result
                    if cache_interval and enriched_backup_objects:
                        store_cached_section(hostname_for_cache, "backup_objects", enriched_backup_objects)
                except VeeamAPIError as e:
                    if args.debug:
                        raise
                    sys.stderr.write(f"Failed to fetch backup objects: {e}\n")

        # Backup objects output based on --backup-mode
        if args.backup_mode and enriched_backup_objects is not None:
            if args.backup_mode == "piggyback-vms":
                # Piggyback to monitored VMs
                output_piggyback_backup_objects(enriched_backup_objects, redactor)
            elif args.backup_mode == "backup-server":
                # Direct services on Veeam server
                output_backup_objects_section(enriched_backup_objects, redactor)

        if "repositories" in args.sections:
            try:
                cache_interval = cache_config.repositories
                cached = load_cached_section(hostname_for_cache, "repositories", cache_interval) if cache_interval else None
                if cached:
                    repos, timestamp = cached
                    output_section("repositories", repos, redactor, timestamp, cache_interval)
                else:
                    repos = api.get_repository_states()
                    timestamp = store_cached_section(hostname_for_cache, "repositories", repos) if cache_interval else None
                    output_section("repositories", repos, redactor, timestamp, cache_interval)
            except VeeamAPIError as e:
                sys.stderr.write(f"Warning: Failed to fetch repositories: {e}\n")

        if "proxies" in args.sections:
            try:
                cache_interval = cache_config.proxies
                cached = load_cached_section(hostname_for_cache, "proxies", cache_interval) if cache_interval else None
                if cached:
                    proxies, timestamp = cached
                    output_section("proxies", proxies, redactor, timestamp, cache_interval)
                else:
                    proxies = api.get_proxy_states()
                    timestamp = store_cached_section(hostname_for_cache, "proxies", proxies) if cache_interval else None
                    output_section("proxies", proxies, redactor, timestamp, cache_interval)
            except VeeamAPIError as e:
                sys.stderr.write(f"Warning: Failed to fetch proxies: {e}\n")

        if "managed_servers" in args.sections:
            try:
                cache_interval = cache_config.managed_servers
                cached = load_cached_section(hostname_for_cache, "managed_servers", cache_interval) if cache_interval else None
                if cached:
                    servers, timestamp = cached
                    output_section("managed_servers", servers, redactor, timestamp, cache_interval)
                else:
                    servers = api.get_managed_servers()
                    timestamp = store_cached_section(hostname_for_cache, "managed_servers", servers) if cache_interval else None
                    output_section("managed_servers", servers, redactor, timestamp, cache_interval)
            except VeeamAPIError as e:
                sys.stderr.write(f"Warning: Failed to fetch managed_servers: {e}\n")

        if "license" in args.sections:
            try:
                cache_interval = cache_config.license
                cached = load_cached_section(hostname_for_cache, "license", cache_interval) if cache_interval else None
                if cached:
                    license_info, timestamp = cached
                    output_section("license", license_info, redactor, timestamp, cache_interval)
                else:
                    license_info = api.get_license()
                    timestamp = store_cached_section(hostname_for_cache, "license", license_info) if cache_interval else None
                    output_section("license", license_info, redactor, timestamp, cache_interval)
            except VeeamAPIError as e:
                sys.stderr.write(f"Warning: Failed to fetch license: {e}\n")

        if "scaleout_repositories" in args.sections:
            try:
                cache_interval = cache_config.scaleout_repositories
                cached = load_cached_section(hostname_for_cache, "scaleout_repositories", cache_interval) if cache_interval else None
                if cached:
                    scaleout_repos, timestamp = cached
                    output_section("scaleout_repositories", scaleout_repos, redactor, timestamp, cache_interval)
                else:
                    scaleout_repos = api.get_scaleout_repositories()
                    timestamp = store_cached_section(hostname_for_cache, "scaleout_repositories", scaleout_repos) if cache_interval else None
                    output_section("scaleout_repositories", scaleout_repos, redactor, timestamp, cache_interval)
            except VeeamAPIError as e:
                sys.stderr.write(f"Warning: Failed to fetch scaleout_repositories: {e}\n")

        if "wan_accelerators" in args.sections:
            try:
                cache_interval = cache_config.wan_accelerators
                cached = load_cached_section(hostname_for_cache, "wan_accelerators", cache_interval) if cache_interval else None
                if cached:
                    wan_accelerators, timestamp = cached
                    output_section("wan_accelerators", wan_accelerators, redactor, timestamp, cache_interval)
                else:
                    wan_accelerators = api.get_wan_accelerators()
                    timestamp = store_cached_section(hostname_for_cache, "wan_accelerators", wan_accelerators) if cache_interval else None
                    output_section("wan_accelerators", wan_accelerators, redactor, timestamp, cache_interval)
            except VeeamAPIError as e:
                sys.stderr.write(f"Warning: Failed to fetch wan_accelerators: {e}\n")

        # Raw malware_events section - only output if NOT using malware_mode
        # (malware_mode outputs enriched data with the same section name)
        if "malware_events" in args.sections and not args.malware_mode:
            try:
                cache_interval = cache_config.malware_events
                cached = load_cached_section(hostname_for_cache, "malware_events", cache_interval) if cache_interval else None
                if cached:
                    malware_events, timestamp = cached
                    output_section("malware_events", malware_events, redactor, timestamp, cache_interval)
                else:
                    malware_events = api.get_malware_events(created_after)
                    timestamp = store_cached_section(hostname_for_cache, "malware_events", malware_events) if cache_interval else None
                    output_section("malware_events", malware_events, redactor, timestamp, cache_interval)
            except VeeamAPIError as e:
                sys.stderr.write(f"Warning: Failed to fetch malware_events: {e}\n")

        # Malware events output based on --malware-mode (combined with backup status)
        if args.malware_mode:
            try:
                cache_interval = cache_config.malware_combined
                cached = load_cached_section(hostname_for_cache, "malware_combined", cache_interval) if cache_interval else None

                if cached:
                    malware_combined, _ = cached
                else:
                    malware_events = api.get_malware_events(created_after)

                    # Reuse enriched backup objects (already fetched above if backup_mode or malware_mode)
                    # This avoids duplicate API calls for restore points
                    malware_combined = enrich_malware_events_with_backup_status(
                        malware_events, enriched_backup_objects or []
                    )
                    store_cached_section(hostname_for_cache, "malware_combined", malware_combined) if cache_interval else None

                if args.malware_mode == "piggyback-hosts":
                    # Piggyback to affected hosts
                    output_piggyback_malware_combined(malware_combined, redactor)
                elif args.malware_mode == "backup-server":
                    # Direct services on Veeam server
                    output_malware_combined(malware_combined, redactor)
            except VeeamAPIError as e:
                if args.debug:
                    raise
                sys.stderr.write(f"Failed to fetch malware events: {e}\n")

    except VeeamAPIError as e:
        # Authentication or critical API error
        print("<<<veeam_rest_error>>>")
        print(str(e))
        sys.stderr.write(f"Error: {e}\n")
        return 1

    except Exception as e:
        sys.stderr.write(f"Unexpected error: {e}\n")
        if args.debug:
            import traceback
            traceback.print_exc()
        return 2

    return 0


if __name__ == "__main__":
    sys.exit(main())
